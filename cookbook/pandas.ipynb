{"metadata":{"colab":{"collapsed_sections":[],"name":"pandas","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Pandas\n\npandas is a software package written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating tabular data and time series. \n","metadata":{"id":"5fCEDCU_qrC0"}},{"cell_type":"markdown","source":"## Learning objective\n\nWe will use an example from a real research project and use pandas to walk through steps that social scientists usually take to handle raw data.\n","metadata":{"id":"rMExucF44s_c"}},{"cell_type":"markdown","source":"## 1. Import pandas\n\nTo begin, let's look at a few ways to import pandas","metadata":{"id":"GJBs_flRovLc"}},{"cell_type":"code","source":"# import specific classes and functions: useful if you know you'll only be needing the most common parts of pandas\nfrom pandas import DataFrame, read_csv\n\n# General syntax to import a package but no functions: \n#   import (package) as (give the package a nickname/alias)\n# Most pandas users will import the entire package and give it the short nickname \"pd\" as follows:\nimport pandas as pd","metadata":{"id":"gJr_9dXGpJ05","trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## 2. Data example\n\nWe are going to use an example dataset retrieved from Stack Overflow.\n\nStack Overflow is a question and answer website. Users can ask and answer questions related to programming. Here is an example question and all the answers:\n\nhttps://stackoverflow.com/questions/4/how-to-convert-a-decimal-to-a-double-in-c\n\nStack Overflow data is publicly available, which allows researchers to observe user behavior in the community.\n\nHere is an overview of the data schema:\n\nhttps://meta.stackexchange.com/questions/2677/database-schema-documentation-for-the-public-data-dump-and-sede\n\nStack Overflow data is commonly used in social science research, especially studies about expertise and knowledge sharing! Here are just a few examples:\n- [How do programmers ask and answer questions on the web?](https://dl.acm.org/doi/abs/10.1145/1985793.1985907)\n- [Discovering Value from Community Activity on Focused Question Answering Sites: A Case Study of Stack Overflow](https://dl.acm.org/doi/abs/10.1145/2339530.2339665)","metadata":{"id":"iUTpAqJ6ykG4"}},{"cell_type":"markdown","source":"## 3. Import csv data\n\nAs mentioned in the introduction, pandas is designed to work with tabular data. Tabluar data is most commonly provided to researchers in a simple spreadsheet format known as csv, which stands for *comma separated values*. Because of this, pandas has a function to load tabular data from a csv file:  `read_csv`. Let us take a look at this function and what inputs it takes.","metadata":{"id":"9c2d00fn3zDE"}},{"cell_type":"code","source":"help(read_csv)","metadata":{"id":"0Qfk96MX10T0","trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Help on function read_csv in module pandas.io.parsers.readers:\n\nread_csv(filepath_or_buffer: 'FilePathOrBuffer', sep=<no_default>, delimiter=None, header='infer', names=<no_default>, index_col=None, usecols=None, squeeze=False, prefix=<no_default>, mangle_dupe_cols=True, dtype: 'DtypeArg | None' = None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, skipfooter=0, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=False, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, cache_dates=True, iterator=False, chunksize=None, compression='infer', thousands=None, decimal: 'str' = '.', lineterminator=None, quotechar='\"', quoting=0, doublequote=True, escapechar=None, comment=None, encoding=None, encoding_errors: 'str | None' = 'strict', dialect=None, error_bad_lines=None, warn_bad_lines=None, on_bad_lines=None, delim_whitespace=False, low_memory=True, memory_map=False, float_precision=None, storage_options: 'StorageOptions' = None)\n    Read a comma-separated values (csv) file into DataFrame.\n    \n    Also supports optionally iterating or breaking of the file\n    into chunks.\n    \n    Additional help can be found in the online docs for\n    `IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n    \n    Parameters\n    ----------\n    filepath_or_buffer : str, path object or file-like object\n        Any valid string path is acceptable. The string could be a URL. Valid\n        URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n        expected. A local file could be: file://localhost/path/to/table.csv.\n    \n        If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n    \n        By file-like object, we refer to objects with a ``read()`` method, such as\n        a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\n    sep : str, default ','\n        Delimiter to use. If sep is None, the C engine cannot automatically detect\n        the separator, but the Python parsing engine can, meaning the latter will\n        be used and automatically detect the separator by Python's builtin sniffer\n        tool, ``csv.Sniffer``. In addition, separators longer than 1 character and\n        different from ``'\\s+'`` will be interpreted as regular expressions and\n        will also force the use of the Python parsing engine. Note that regex\n        delimiters are prone to ignoring quoted data. Regex example: ``'\\r\\t'``.\n    delimiter : str, default ``None``\n        Alias for sep.\n    header : int, list of int, default 'infer'\n        Row number(s) to use as the column names, and the start of the\n        data.  Default behavior is to infer the column names: if no names\n        are passed the behavior is identical to ``header=0`` and column\n        names are inferred from the first line of the file, if column\n        names are passed explicitly then the behavior is identical to\n        ``header=None``. Explicitly pass ``header=0`` to be able to\n        replace existing names. The header can be a list of integers that\n        specify row locations for a multi-index on the columns\n        e.g. [0,1,3]. Intervening rows that are not specified will be\n        skipped (e.g. 2 in this example is skipped). Note that this\n        parameter ignores commented lines and empty lines if\n        ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n        data rather than the first line of the file.\n    names : array-like, optional\n        List of column names to use. If the file contains a header row,\n        then you should explicitly pass ``header=0`` to override the column names.\n        Duplicates in this list are not allowed.\n    index_col : int, str, sequence of int / str, or False, default ``None``\n      Column(s) to use as the row labels of the ``DataFrame``, either given as\n      string name or column index. If a sequence of int / str is given, a\n      MultiIndex is used.\n    \n      Note: ``index_col=False`` can be used to force pandas to *not* use the first\n      column as the index, e.g. when you have a malformed file with delimiters at\n      the end of each line.\n    usecols : list-like or callable, optional\n        Return a subset of the columns. If list-like, all elements must either\n        be positional (i.e. integer indices into the document columns) or strings\n        that correspond to column names provided either by the user in `names` or\n        inferred from the document header row(s). For example, a valid list-like\n        `usecols` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n        Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n        To instantiate a DataFrame from ``data`` with element order preserved use\n        ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]`` for columns\n        in ``['foo', 'bar']`` order or\n        ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n        for ``['bar', 'foo']`` order.\n    \n        If callable, the callable function will be evaluated against the column\n        names, returning names where the callable function evaluates to True. An\n        example of a valid callable argument would be ``lambda x: x.upper() in\n        ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n        parsing time and lower memory usage.\n    squeeze : bool, default False\n        If the parsed data only contains one column then return a Series.\n    prefix : str, optional\n        Prefix to add to column numbers when no header, e.g. 'X' for X0, X1, ...\n    mangle_dupe_cols : bool, default True\n        Duplicate columns will be specified as 'X', 'X.1', ...'X.N', rather than\n        'X'...'X'. Passing in False will cause data to be overwritten if there\n        are duplicate names in the columns.\n    dtype : Type name or dict of column -> type, optional\n        Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32,\n        'c': 'Int64'}\n        Use `str` or `object` together with suitable `na_values` settings\n        to preserve and not interpret dtype.\n        If converters are specified, they will be applied INSTEAD\n        of dtype conversion.\n    engine : {'c', 'python'}, optional\n        Parser engine to use. The C engine is faster while the python engine is\n        currently more feature-complete.\n    converters : dict, optional\n        Dict of functions for converting values in certain columns. Keys can either\n        be integers or column labels.\n    true_values : list, optional\n        Values to consider as True.\n    false_values : list, optional\n        Values to consider as False.\n    skipinitialspace : bool, default False\n        Skip spaces after delimiter.\n    skiprows : list-like, int or callable, optional\n        Line numbers to skip (0-indexed) or number of lines to skip (int)\n        at the start of the file.\n    \n        If callable, the callable function will be evaluated against the row\n        indices, returning True if the row should be skipped and False otherwise.\n        An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\n    skipfooter : int, default 0\n        Number of lines at bottom of file to skip (Unsupported with engine='c').\n    nrows : int, optional\n        Number of rows of file to read. Useful for reading pieces of large files.\n    na_values : scalar, str, list-like, or dict, optional\n        Additional strings to recognize as NA/NaN. If dict passed, specific\n        per-column NA values.  By default the following values are interpreted as\n        NaN: '', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan',\n        '1.#IND', '1.#QNAN', '<NA>', 'N/A', 'NA', 'NULL', 'NaN', 'n/a',\n        'nan', 'null'.\n    keep_default_na : bool, default True\n        Whether or not to include the default NaN values when parsing the data.\n        Depending on whether `na_values` is passed in, the behavior is as follows:\n    \n        * If `keep_default_na` is True, and `na_values` are specified, `na_values`\n          is appended to the default NaN values used for parsing.\n        * If `keep_default_na` is True, and `na_values` are not specified, only\n          the default NaN values are used for parsing.\n        * If `keep_default_na` is False, and `na_values` are specified, only\n          the NaN values specified `na_values` are used for parsing.\n        * If `keep_default_na` is False, and `na_values` are not specified, no\n          strings will be parsed as NaN.\n    \n        Note that if `na_filter` is passed in as False, the `keep_default_na` and\n        `na_values` parameters will be ignored.\n    na_filter : bool, default True\n        Detect missing value markers (empty strings and the value of na_values). In\n        data without any NAs, passing na_filter=False can improve the performance\n        of reading a large file.\n    verbose : bool, default False\n        Indicate number of NA values placed in non-numeric columns.\n    skip_blank_lines : bool, default True\n        If True, skip over blank lines rather than interpreting as NaN values.\n    parse_dates : bool or list of int or names or list of lists or dict, default False\n        The behavior is as follows:\n    \n        * boolean. If True -> try parsing the index.\n        * list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\n          each as a separate date column.\n        * list of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as\n          a single date column.\n        * dict, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call\n          result 'foo'\n    \n        If a column or index cannot be represented as an array of datetimes,\n        say because of an unparsable value or a mixture of timezones, the column\n        or index will be returned unaltered as an object data type. For\n        non-standard datetime parsing, use ``pd.to_datetime`` after\n        ``pd.read_csv``. To parse an index or column with a mixture of timezones,\n        specify ``date_parser`` to be a partially-applied\n        :func:`pandas.to_datetime` with ``utc=True``. See\n        :ref:`io.csv.mixed_timezones` for more.\n    \n        Note: A fast-path exists for iso8601-formatted dates.\n    infer_datetime_format : bool, default False\n        If True and `parse_dates` is enabled, pandas will attempt to infer the\n        format of the datetime strings in the columns, and if it can be inferred,\n        switch to a faster method of parsing them. In some cases this can increase\n        the parsing speed by 5-10x.\n    keep_date_col : bool, default False\n        If True and `parse_dates` specifies combining multiple columns then\n        keep the original columns.\n    date_parser : function, optional\n        Function to use for converting a sequence of string columns to an array of\n        datetime instances. The default uses ``dateutil.parser.parser`` to do the\n        conversion. Pandas will try to call `date_parser` in three different ways,\n        advancing to the next if an exception occurs: 1) Pass one or more arrays\n        (as defined by `parse_dates`) as arguments; 2) concatenate (row-wise) the\n        string values from the columns defined by `parse_dates` into a single array\n        and pass that; and 3) call `date_parser` once for each row using one or\n        more strings (corresponding to the columns defined by `parse_dates`) as\n        arguments.\n    dayfirst : bool, default False\n        DD/MM format dates, international and European format.\n    cache_dates : bool, default True\n        If True, use a cache of unique, converted dates to apply the datetime\n        conversion. May produce significant speed-up when parsing duplicate\n        date strings, especially ones with timezone offsets.\n    \n        .. versionadded:: 0.25.0\n    iterator : bool, default False\n        Return TextFileReader object for iteration or getting chunks with\n        ``get_chunk()``.\n    \n        .. versionchanged:: 1.2\n    \n           ``TextFileReader`` is a context manager.\n    chunksize : int, optional\n        Return TextFileReader object for iteration.\n        See the `IO Tools docs\n        <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n        for more information on ``iterator`` and ``chunksize``.\n    \n        .. versionchanged:: 1.2\n    \n           ``TextFileReader`` is a context manager.\n    compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n        For on-the-fly decompression of on-disk data. If 'infer' and\n        `filepath_or_buffer` is path-like, then detect compression from the\n        following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no\n        decompression). If using 'zip', the ZIP file must contain only one data\n        file to be read in. Set to None for no decompression.\n    thousands : str, optional\n        Thousands separator.\n    decimal : str, default '.'\n        Character to recognize as decimal point (e.g. use ',' for European data).\n    lineterminator : str (length 1), optional\n        Character to break file into lines. Only valid with C parser.\n    quotechar : str (length 1), optional\n        The character used to denote the start and end of a quoted item. Quoted\n        items can include the delimiter and it will be ignored.\n    quoting : int or csv.QUOTE_* instance, default 0\n        Control field quoting behavior per ``csv.QUOTE_*`` constants. Use one of\n        QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).\n    doublequote : bool, default ``True``\n       When quotechar is specified and quoting is not ``QUOTE_NONE``, indicate\n       whether or not to interpret two consecutive quotechar elements INSIDE a\n       field as a single ``quotechar`` element.\n    escapechar : str (length 1), optional\n        One-character string used to escape other characters.\n    comment : str, optional\n        Indicates remainder of line should not be parsed. If found at the beginning\n        of a line, the line will be ignored altogether. This parameter must be a\n        single character. Like empty lines (as long as ``skip_blank_lines=True``),\n        fully commented lines are ignored by the parameter `header` but not by\n        `skiprows`. For example, if ``comment='#'``, parsing\n        ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in 'a,b,c' being\n        treated as the header.\n    encoding : str, optional\n        Encoding to use for UTF when reading/writing (ex. 'utf-8'). `List of Python\n        standard encodings\n        <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n    \n        .. versionchanged:: 1.2\n    \n           When ``encoding`` is ``None``, ``errors=\"replace\"`` is passed to\n           ``open()``. Otherwise, ``errors=\"strict\"`` is passed to ``open()``.\n           This behavior was previously only the case for ``engine=\"python\"``.\n    \n        .. versionchanged:: 1.3.0\n    \n           ``encoding_errors`` is a new argument. ``encoding`` has no longer an\n           influence on how encoding errors are handled.\n    \n    encoding_errors : str, optional, default \"strict\"\n        How encoding errors are treated. `List of possible values\n        <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n    \n        .. versionadded:: 1.3.0\n    \n    dialect : str or csv.Dialect, optional\n        If provided, this parameter will override values (default or not) for the\n        following parameters: `delimiter`, `doublequote`, `escapechar`,\n        `skipinitialspace`, `quotechar`, and `quoting`. If it is necessary to\n        override values, a ParserWarning will be issued. See csv.Dialect\n        documentation for more details.\n    error_bad_lines : bool, default ``None``\n        Lines with too many fields (e.g. a csv line with too many commas) will by\n        default cause an exception to be raised, and no DataFrame will be returned.\n        If False, then these \"bad lines\" will be dropped from the DataFrame that is\n        returned.\n    \n        .. deprecated:: 1.3.0\n           The ``on_bad_lines`` parameter should be used instead to specify behavior upon\n           encountering a bad line instead.\n    warn_bad_lines : bool, default ``None``\n        If error_bad_lines is False, and warn_bad_lines is True, a warning for each\n        \"bad line\" will be output.\n    \n        .. deprecated:: 1.3.0\n           The ``on_bad_lines`` parameter should be used instead to specify behavior upon\n           encountering a bad line instead.\n    on_bad_lines : {'error', 'warn', 'skip'}, default 'error'\n        Specifies what to do upon encountering a bad line (a line with too many fields).\n        Allowed values are :\n    \n            - 'error', raise an Exception when a bad line is encountered.\n            - 'warn', raise a warning when a bad line is encountered and skip that line.\n            - 'skip', skip bad lines without raising or warning when they are encountered.\n    \n        .. versionadded:: 1.3.0\n    \n    delim_whitespace : bool, default False\n        Specifies whether or not whitespace (e.g. ``' '`` or ``'    '``) will be\n        used as the sep. Equivalent to setting ``sep='\\s+'``. If this option\n        is set to True, nothing should be passed in for the ``delimiter``\n        parameter.\n    low_memory : bool, default True\n        Internally process the file in chunks, resulting in lower memory use\n        while parsing, but possibly mixed type inference.  To ensure no mixed\n        types either set False, or specify the type with the `dtype` parameter.\n        Note that the entire file is read into a single DataFrame regardless,\n        use the `chunksize` or `iterator` parameter to return the data in chunks.\n        (Only valid with C parser).\n    memory_map : bool, default False\n        If a filepath is provided for `filepath_or_buffer`, map the file object\n        directly onto memory and access the data directly from there. Using this\n        option can improve performance because there is no longer any I/O overhead.\n    float_precision : str, optional\n        Specifies which converter the C engine should use for floating-point\n        values. The options are ``None`` or 'high' for the ordinary converter,\n        'legacy' for the original lower precision pandas converter, and\n        'round_trip' for the round-trip converter.\n    \n        .. versionchanged:: 1.2\n    \n    storage_options : dict, optional\n        Extra options that make sense for a particular storage connection, e.g.\n        host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n        are forwarded to ``urllib`` as header options. For other URLs (e.g.\n        starting with \"s3://\", and \"gcs://\") the key-value pairs are forwarded to\n        ``fsspec``. Please see ``fsspec`` and ``urllib`` for more details.\n    \n        .. versionadded:: 1.2\n    \n    Returns\n    -------\n    DataFrame or TextParser\n        A comma-separated values (csv) file is returned as two-dimensional\n        data structure with labeled axes.\n    \n    See Also\n    --------\n    DataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\n    read_csv : Read a comma-separated values (csv) file into DataFrame.\n    read_fwf : Read a table of fixed-width formatted lines into DataFrame.\n    \n    Examples\n    --------\n    >>> pd.read_csv('data.csv')  # doctest: +SKIP\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Common parameters in **read_csv**\n\nread_csv(filepath, sep= , header= , usecols=None)\n\n- filepath_or_buffer : str, path object or file-like object\n   \n- sep : str, default ','\n    \n- header : int, list of int\n - Row number(s) to use as the column names, and the start of the data.  \n    \n- names : array-like, optional\n - List of column names to use, if the file contains a header row\n   \n- usecols : list-like or callable\n - Return a subset of the columns. \n - For example, a valid list-like\n    `usecols` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n   \n","metadata":{"id":"OL5f9X4l1XLa"}},{"cell_type":"markdown","source":"As we can see from the parameters description, we need to give `read_csv` a path to the csv file we want to load. Here, we will load the example StackOverflow data, which is contained in the file \"so_question.csv\" in our GitHub repo.","metadata":{}},{"cell_type":"code","source":"# The following path is valid for our Binder setup. If you are running this notebook in your local machine instead and have the file downloaded to a different location, you may need to change this path.\nLocation = './data/so_question.csv'\ndf = pd.read_csv(Location)","metadata":{"id":"7X7PRKhX4kJV","trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"`read_csv` loads the data into a structure known as a **DataFrame**, as we can see if we check the type of the variable `df` that we created:","metadata":{}},{"cell_type":"code","source":"type(df)","metadata":{"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"pandas.core.frame.DataFrame"},"metadata":{}}]},{"cell_type":"markdown","source":"Think of a **DataFrame** as a table or spreadsheet that you can work with inside your Python code! Like any table, it contains data organized into rows and columns.\n\nNow you might be wondering what the data actually looks like. The best way to find out is to use the `display` function, which will show a sample of data from the DataFrame. This is always the first thing you should do after loading a new dataset in pandas!","metadata":{}},{"cell_type":"code","source":"display(df)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"NGGMN_Ti28h3","outputId":"cf999efc-4b69-41db-b99e-f933cc7c81d9","trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"       24752851  2014-07-15 07:58:49          .htaccess  2014   7    24768174  \\\n0      39926330  2016-10-07 21:55:43          .htaccess  2016  10         NaN   \n1      26762140  2014-11-05 16:26:24          .htaccess  2014  11  26762882.0   \n2      29889423  2015-04-27 07:14:00          .htaccess  2015   4         NaN   \n3      10582676  2012-05-14 11:43:27          .htaccess  2012   5         NaN   \n4      24544084  2014-07-03 02:39:25          .htaccess  2014   7         NaN   \n...         ...                  ...                ...   ...  ..         ...   \n60564  39093361  2016-08-23 05:55:50    zurb-foundation  2016   8         NaN   \n60565  37248756  2016-05-16 07:25:11  zurb-foundation-6  2016   5  37251132.0   \n60566  16264645  2013-04-28 15:50:01              zxing  2013   4  16264687.0   \n60567  30389191  2015-05-22 05:46:07              zxing  2015   5         NaN   \n60568  26181174  2014-10-03 14:44:59              zxing  2014  10         NaN   \n\n          509916  \n0      5281549.0  \n1      2091861.0  \n2       589973.0  \n3      1326187.0  \n4      1448031.0  \n...          ...  \n60564  2729887.0  \n60565  1148107.0  \n60566  2102871.0  \n60567   571433.0  \n60568  1491212.0  \n\n[60569 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>24752851</th>\n      <th>2014-07-15 07:58:49</th>\n      <th>.htaccess</th>\n      <th>2014</th>\n      <th>7</th>\n      <th>24768174</th>\n      <th>509916</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>39926330</td>\n      <td>2016-10-07 21:55:43</td>\n      <td>.htaccess</td>\n      <td>2016</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>5281549.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>26762140</td>\n      <td>2014-11-05 16:26:24</td>\n      <td>.htaccess</td>\n      <td>2014</td>\n      <td>11</td>\n      <td>26762882.0</td>\n      <td>2091861.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>29889423</td>\n      <td>2015-04-27 07:14:00</td>\n      <td>.htaccess</td>\n      <td>2015</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>589973.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10582676</td>\n      <td>2012-05-14 11:43:27</td>\n      <td>.htaccess</td>\n      <td>2012</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>1326187.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>24544084</td>\n      <td>2014-07-03 02:39:25</td>\n      <td>.htaccess</td>\n      <td>2014</td>\n      <td>7</td>\n      <td>NaN</td>\n      <td>1448031.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>60564</th>\n      <td>39093361</td>\n      <td>2016-08-23 05:55:50</td>\n      <td>zurb-foundation</td>\n      <td>2016</td>\n      <td>8</td>\n      <td>NaN</td>\n      <td>2729887.0</td>\n    </tr>\n    <tr>\n      <th>60565</th>\n      <td>37248756</td>\n      <td>2016-05-16 07:25:11</td>\n      <td>zurb-foundation-6</td>\n      <td>2016</td>\n      <td>5</td>\n      <td>37251132.0</td>\n      <td>1148107.0</td>\n    </tr>\n    <tr>\n      <th>60566</th>\n      <td>16264645</td>\n      <td>2013-04-28 15:50:01</td>\n      <td>zxing</td>\n      <td>2013</td>\n      <td>4</td>\n      <td>16264687.0</td>\n      <td>2102871.0</td>\n    </tr>\n    <tr>\n      <th>60567</th>\n      <td>30389191</td>\n      <td>2015-05-22 05:46:07</td>\n      <td>zxing</td>\n      <td>2015</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>571433.0</td>\n    </tr>\n    <tr>\n      <th>60568</th>\n      <td>26181174</td>\n      <td>2014-10-03 14:44:59</td>\n      <td>zxing</td>\n      <td>2014</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>1491212.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>60569 rows × 7 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"This brings us to the first problem of the exercise. The `read_csv` function treated the first record in the csv file as the header names. This is obviously not correct since the text file did not provide us with header names; instead, we can see that the first row looks like real data.\n\nTo correct this we will pass the `header` parameter to the read_csv function and set it to **None** (means null in python).","metadata":{"id":"in0J3EXx3eJ3"}},{"cell_type":"code","source":"df = pd.read_csv(Location, header=None)\ndisplay(df)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"OxDMJ17V3hW_","outputId":"d9ec3d5a-c475-4639-9d0a-fd62e1dca511"},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>24752851</td>\n      <td>2014-07-15 07:58:49</td>\n      <td>.htaccess</td>\n      <td>2014</td>\n      <td>7</td>\n      <td>24768174.0</td>\n      <td>509916.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>39926330</td>\n      <td>2016-10-07 21:55:43</td>\n      <td>.htaccess</td>\n      <td>2016</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>5281549.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>26762140</td>\n      <td>2014-11-05 16:26:24</td>\n      <td>.htaccess</td>\n      <td>2014</td>\n      <td>11</td>\n      <td>26762882.0</td>\n      <td>2091861.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>29889423</td>\n      <td>2015-04-27 07:14:00</td>\n      <td>.htaccess</td>\n      <td>2015</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>589973.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10582676</td>\n      <td>2012-05-14 11:43:27</td>\n      <td>.htaccess</td>\n      <td>2012</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>1326187.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>60565</th>\n      <td>39093361</td>\n      <td>2016-08-23 05:55:50</td>\n      <td>zurb-foundation</td>\n      <td>2016</td>\n      <td>8</td>\n      <td>NaN</td>\n      <td>2729887.0</td>\n    </tr>\n    <tr>\n      <th>60566</th>\n      <td>37248756</td>\n      <td>2016-05-16 07:25:11</td>\n      <td>zurb-foundation-6</td>\n      <td>2016</td>\n      <td>5</td>\n      <td>37251132.0</td>\n      <td>1148107.0</td>\n    </tr>\n    <tr>\n      <th>60567</th>\n      <td>16264645</td>\n      <td>2013-04-28 15:50:01</td>\n      <td>zxing</td>\n      <td>2013</td>\n      <td>4</td>\n      <td>16264687.0</td>\n      <td>2102871.0</td>\n    </tr>\n    <tr>\n      <th>60568</th>\n      <td>30389191</td>\n      <td>2015-05-22 05:46:07</td>\n      <td>zxing</td>\n      <td>2015</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>571433.0</td>\n    </tr>\n    <tr>\n      <th>60569</th>\n      <td>26181174</td>\n      <td>2014-10-03 14:44:59</td>\n      <td>zxing</td>\n      <td>2014</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>1491212.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>60570 rows × 7 columns</p>\n</div>","text/plain":"              0                    1                  2     3   4           5  \\\n0      24752851  2014-07-15 07:58:49          .htaccess  2014   7  24768174.0   \n1      39926330  2016-10-07 21:55:43          .htaccess  2016  10         NaN   \n2      26762140  2014-11-05 16:26:24          .htaccess  2014  11  26762882.0   \n3      29889423  2015-04-27 07:14:00          .htaccess  2015   4         NaN   \n4      10582676  2012-05-14 11:43:27          .htaccess  2012   5         NaN   \n...         ...                  ...                ...   ...  ..         ...   \n60565  39093361  2016-08-23 05:55:50    zurb-foundation  2016   8         NaN   \n60566  37248756  2016-05-16 07:25:11  zurb-foundation-6  2016   5  37251132.0   \n60567  16264645  2013-04-28 15:50:01              zxing  2013   4  16264687.0   \n60568  30389191  2015-05-22 05:46:07              zxing  2015   5         NaN   \n60569  26181174  2014-10-03 14:44:59              zxing  2014  10         NaN   \n\n               6  \n0       509916.0  \n1      5281549.0  \n2      2091861.0  \n3       589973.0  \n4      1326187.0  \n...          ...  \n60565  2729887.0  \n60566  1148107.0  \n60567  2102871.0  \n60568   571433.0  \n60569  1491212.0  \n\n[60570 rows x 7 columns]"},"metadata":{}}]},{"cell_type":"markdown","source":"We can see that all 10000 rows of the csv file are now correctly treated as real data. Since no header names were given, pandas automatically just numbers the headers starting from 0. This is obviously not very useful, usually you want your columns to have meaningful names describing what kind of data they contain. In this case, the documentation for the StackOverflow dataset tells us what the columns are: \n- The first column is an ID\n- The second column is a timestamp\n- The third column is a topic tag\n- The fourth column is a year\n- The fifth column is a month\n- The sixth column is the ID of the accepted answer, if any\n- The seventh column is the ID of the author\n\nNow we want to tell pandas to use column names that are descriptive of this data. We can do this by passing a list of column names to the parameter `names`. This also lets us omit the `header` parameter.","metadata":{"id":"L2wafQlD3_S5"}},{"cell_type":"code","source":"df = pd.read_csv(Location, names=['id', 'creation_time', 'tag', 'year','month', 'accepted_id','owner_user_id'])\ndisplay(df)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"7d3S5YmR4ERa","outputId":"c46193a1-6ec5-46e8-f637-2668885ddd69"},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>creation_time</th>\n      <th>tag</th>\n      <th>year</th>\n      <th>month</th>\n      <th>accepted_id</th>\n      <th>owner_user_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>24752851</td>\n      <td>2014-07-15 07:58:49</td>\n      <td>.htaccess</td>\n      <td>2014</td>\n      <td>7</td>\n      <td>24768174.0</td>\n      <td>509916.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>39926330</td>\n      <td>2016-10-07 21:55:43</td>\n      <td>.htaccess</td>\n      <td>2016</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>5281549.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>26762140</td>\n      <td>2014-11-05 16:26:24</td>\n      <td>.htaccess</td>\n      <td>2014</td>\n      <td>11</td>\n      <td>26762882.0</td>\n      <td>2091861.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>29889423</td>\n      <td>2015-04-27 07:14:00</td>\n      <td>.htaccess</td>\n      <td>2015</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>589973.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10582676</td>\n      <td>2012-05-14 11:43:27</td>\n      <td>.htaccess</td>\n      <td>2012</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>1326187.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>60565</th>\n      <td>39093361</td>\n      <td>2016-08-23 05:55:50</td>\n      <td>zurb-foundation</td>\n      <td>2016</td>\n      <td>8</td>\n      <td>NaN</td>\n      <td>2729887.0</td>\n    </tr>\n    <tr>\n      <th>60566</th>\n      <td>37248756</td>\n      <td>2016-05-16 07:25:11</td>\n      <td>zurb-foundation-6</td>\n      <td>2016</td>\n      <td>5</td>\n      <td>37251132.0</td>\n      <td>1148107.0</td>\n    </tr>\n    <tr>\n      <th>60567</th>\n      <td>16264645</td>\n      <td>2013-04-28 15:50:01</td>\n      <td>zxing</td>\n      <td>2013</td>\n      <td>4</td>\n      <td>16264687.0</td>\n      <td>2102871.0</td>\n    </tr>\n    <tr>\n      <th>60568</th>\n      <td>30389191</td>\n      <td>2015-05-22 05:46:07</td>\n      <td>zxing</td>\n      <td>2015</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>571433.0</td>\n    </tr>\n    <tr>\n      <th>60569</th>\n      <td>26181174</td>\n      <td>2014-10-03 14:44:59</td>\n      <td>zxing</td>\n      <td>2014</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>1491212.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>60570 rows × 7 columns</p>\n</div>","text/plain":"             id        creation_time                tag  year  month  \\\n0      24752851  2014-07-15 07:58:49          .htaccess  2014      7   \n1      39926330  2016-10-07 21:55:43          .htaccess  2016     10   \n2      26762140  2014-11-05 16:26:24          .htaccess  2014     11   \n3      29889423  2015-04-27 07:14:00          .htaccess  2015      4   \n4      10582676  2012-05-14 11:43:27          .htaccess  2012      5   \n...         ...                  ...                ...   ...    ...   \n60565  39093361  2016-08-23 05:55:50    zurb-foundation  2016      8   \n60566  37248756  2016-05-16 07:25:11  zurb-foundation-6  2016      5   \n60567  16264645  2013-04-28 15:50:01              zxing  2013      4   \n60568  30389191  2015-05-22 05:46:07              zxing  2015      5   \n60569  26181174  2014-10-03 14:44:59              zxing  2014     10   \n\n       accepted_id  owner_user_id  \n0       24768174.0       509916.0  \n1              NaN      5281549.0  \n2       26762882.0      2091861.0  \n3              NaN       589973.0  \n4              NaN      1326187.0  \n...            ...            ...  \n60565          NaN      2729887.0  \n60566   37251132.0      1148107.0  \n60567   16264687.0      2102871.0  \n60568          NaN       571433.0  \n60569          NaN      1491212.0  \n\n[60570 rows x 7 columns]"},"metadata":{}}]},{"cell_type":"markdown","source":"Notice the leftmost column is special: it does not have a name and simply contains consecutive numbers [0,1,2,3,4,...]. This is the *index* of the DataFrame; you can think of these as the row numbers in an Excel file. This index is also similar to the primary key of a sql table with the exception that an index is allowed to have duplicates.\n\nOne last thing we want to point out before we move on: notice that in the accepted_id column, there seems to be a mix of data: some look like numerical IDs, but others say \"NaN\". NaN is short for \"Not A Number\" and is pandas' way of representing *missing data*. Think of NaNs as equivalent to an empty cell in an Excel file. The meaning of missing data depends on the context of the dataset. In this case, since this data is from Stack Overflow and the \"accepted_id\" column represents the ID of an accepted answer, we can infer that in this case missing data means that the question never received an accepted answer. In research, we often want to do something special to handle missing data; we will return to this later in the workshop.","metadata":{"id":"iwv6zshx5ylG"}},{"cell_type":"markdown","source":"## 4. Checking the structure of the data\nUsually we first do a couple of sanity checks to make sure the data we imported is in good shape and understand its high level structure. \n\nUsually we make sure the data type of each variable is sensible, check the total number of observations, and the number of variables in the data set.\n\nLet's take a look at the example data set. First, we'll check data types using the `dtypes` variable.\n","metadata":{"id":"OwuxHmxllTwN"}},{"cell_type":"code","source":"# Check data type of the columns\ndf.dtypes","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yCC64Uqy6nAY","outputId":"eea2432a-ca08-4563-c614-c1d78288af19"},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"id                 int64\ncreation_time     object\ntag               object\nyear               int64\nmonth              int64\naccepted_id      float64\nowner_user_id    float64\ndtype: object"},"metadata":{}}]},{"cell_type":"code","source":"# Check data type of a specific column\ndf.accepted_id.dtype","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OI7fDnZh6tZ5","outputId":"afda98ae-6e03-4427-b376-d15cdc6295a7"},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"dtype('float64')"},"metadata":{}}]},{"cell_type":"markdown","source":"Next we'll check the number of observations (rows) in the dataset. This can be done by examining the length of the DataFrame's *index*:","metadata":{"id":"NmbZVWO2BSLU"}},{"cell_type":"code","source":"len(df.index)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QIoJ_maABVOg","outputId":"6fb2c9f4-e5d3-4ab2-f681-4d6a52b9941b"},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"60570"},"metadata":{}}]},{"cell_type":"markdown","source":"Similarly, we can check the number of variables (columns) in the dataset by examining the length of the DataFrame's *columns* list:","metadata":{"id":"IWQpJOKXBkCm"}},{"cell_type":"code","source":"len(df.columns)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iwu71C2hBmVG","outputId":"3a6a8eb0-7883-46be-a0f7-575f5cb94e97"},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"7"},"metadata":{}}]},{"cell_type":"markdown","source":"## 5. Slicing and Indexing\nUsually, we will not be working with an entire data set all at the same time. Instead, we usually want to pick out specific rows or columns to analyze and work with. In pandas, this can be done using *slicing and indexing*.","metadata":{"id":"g0z8mSwz6ZvH"}},{"cell_type":"markdown","source":"The most basic indexing operation is selecting a specific column. This can be done using standard Python indexing syntax (square brackets) and giving it the name of the column you want. For example, the following code will select the \"id\" column:","metadata":{}},{"cell_type":"code","source":"id_column = df['id']\ndisplay(id_column)","metadata":{},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"0        24752851\n1        39926330\n2        26762140\n3        29889423\n4        10582676\n           ...   \n60565    39093361\n60566    37248756\n60567    16264645\n60568    30389191\n60569    26181174\nName: id, Length: 60570, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"The syntax for selecting a row is similar, except that you need to tell pandas that you are trying to access a row, since indexing defaults to columns. To do this, use the `loc` variable:","metadata":{}},{"cell_type":"code","source":"fifth_row = df.loc[4] # remember the index starts from 0, so the fifth row is at index 4\ndisplay(fifth_row)","metadata":{},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"id                          10582676\ncreation_time    2012-05-14 11:43:27\ntag                        .htaccess\nyear                            2012\nmonth                              5\naccepted_id                      NaN\nowner_user_id              1326187.0\nName: 4, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":"Both row indexing and column indexing allow you to select multiple rows or columns at once. To do this, you can provide a **list** of row indices or column names, instead of just one:","metadata":{}},{"cell_type":"code","source":"multi_columns = df[['id', 'creation_time']]\ndisplay(multi_columns)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"WvDmj7y6HwJi","outputId":"7b6483cd-6976-457e-9a5f-7909278f0269"},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>creation_time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>24752851</td>\n      <td>2014-07-15 07:58:49</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>39926330</td>\n      <td>2016-10-07 21:55:43</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>26762140</td>\n      <td>2014-11-05 16:26:24</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>29889423</td>\n      <td>2015-04-27 07:14:00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10582676</td>\n      <td>2012-05-14 11:43:27</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>60565</th>\n      <td>39093361</td>\n      <td>2016-08-23 05:55:50</td>\n    </tr>\n    <tr>\n      <th>60566</th>\n      <td>37248756</td>\n      <td>2016-05-16 07:25:11</td>\n    </tr>\n    <tr>\n      <th>60567</th>\n      <td>16264645</td>\n      <td>2013-04-28 15:50:01</td>\n    </tr>\n    <tr>\n      <th>60568</th>\n      <td>30389191</td>\n      <td>2015-05-22 05:46:07</td>\n    </tr>\n    <tr>\n      <th>60569</th>\n      <td>26181174</td>\n      <td>2014-10-03 14:44:59</td>\n    </tr>\n  </tbody>\n</table>\n<p>60570 rows × 2 columns</p>\n</div>","text/plain":"             id        creation_time\n0      24752851  2014-07-15 07:58:49\n1      39926330  2016-10-07 21:55:43\n2      26762140  2014-11-05 16:26:24\n3      29889423  2015-04-27 07:14:00\n4      10582676  2012-05-14 11:43:27\n...         ...                  ...\n60565  39093361  2016-08-23 05:55:50\n60566  37248756  2016-05-16 07:25:11\n60567  16264645  2013-04-28 15:50:01\n60568  30389191  2015-05-22 05:46:07\n60569  26181174  2014-10-03 14:44:59\n\n[60570 rows x 2 columns]"},"metadata":{}}]},{"cell_type":"code","source":"multi_rows = df.loc[[0,1,2,3]]\ndisplay(multi_rows)","metadata":{},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>creation_time</th>\n      <th>tag</th>\n      <th>year</th>\n      <th>month</th>\n      <th>accepted_id</th>\n      <th>owner_user_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>24752851</td>\n      <td>2014-07-15 07:58:49</td>\n      <td>.htaccess</td>\n      <td>2014</td>\n      <td>7</td>\n      <td>24768174.0</td>\n      <td>509916.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>39926330</td>\n      <td>2016-10-07 21:55:43</td>\n      <td>.htaccess</td>\n      <td>2016</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>5281549.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>26762140</td>\n      <td>2014-11-05 16:26:24</td>\n      <td>.htaccess</td>\n      <td>2014</td>\n      <td>11</td>\n      <td>26762882.0</td>\n      <td>2091861.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>29889423</td>\n      <td>2015-04-27 07:14:00</td>\n      <td>.htaccess</td>\n      <td>2015</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>589973.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"         id        creation_time        tag  year  month  accepted_id  \\\n0  24752851  2014-07-15 07:58:49  .htaccess  2014      7   24768174.0   \n1  39926330  2016-10-07 21:55:43  .htaccess  2016     10          NaN   \n2  26762140  2014-11-05 16:26:24  .htaccess  2014     11   26762882.0   \n3  29889423  2015-04-27 07:14:00  .htaccess  2015      4          NaN   \n\n   owner_user_id  \n0       509916.0  \n1      5281549.0  \n2      2091861.0  \n3       589973.0  "},"metadata":{}}]},{"cell_type":"markdown","source":"Since row indices are numerical, python's **slicing** syntax can also be used on them to select a range of rows. This can simplify your code a lot because you can specify a range rather than writing out full list of indices! For example, the following code is equivalent to the previous code:","metadata":{}},{"cell_type":"code","source":"multi_rows = df.loc[0:3] # important difference from regular python: pandas slices include *both* indices!\ndisplay(multi_rows)","metadata":{},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>creation_time</th>\n      <th>tag</th>\n      <th>year</th>\n      <th>month</th>\n      <th>accepted_id</th>\n      <th>owner_user_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>24752851</td>\n      <td>2014-07-15 07:58:49</td>\n      <td>.htaccess</td>\n      <td>2014</td>\n      <td>7</td>\n      <td>24768174.0</td>\n      <td>509916.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>39926330</td>\n      <td>2016-10-07 21:55:43</td>\n      <td>.htaccess</td>\n      <td>2016</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>5281549.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>26762140</td>\n      <td>2014-11-05 16:26:24</td>\n      <td>.htaccess</td>\n      <td>2014</td>\n      <td>11</td>\n      <td>26762882.0</td>\n      <td>2091861.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>29889423</td>\n      <td>2015-04-27 07:14:00</td>\n      <td>.htaccess</td>\n      <td>2015</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>589973.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"         id        creation_time        tag  year  month  accepted_id  \\\n0  24752851  2014-07-15 07:58:49  .htaccess  2014      7   24768174.0   \n1  39926330  2016-10-07 21:55:43  .htaccess  2016     10          NaN   \n2  26762140  2014-11-05 16:26:24  .htaccess  2014     11   26762882.0   \n3  29889423  2015-04-27 07:14:00  .htaccess  2015      4          NaN   \n\n   owner_user_id  \n0       509916.0  \n1      5281549.0  \n2      2091861.0  \n3       589973.0  "},"metadata":{}}]},{"cell_type":"markdown","source":"Additionally, we may sometimes want to pick out a **single** value from the DataFrame. For example, suppose we want to find the question ID in the fifth row. We can do this by giving both a row and column to the `loc` indexer. Note that the row and column **must** be given in that order (you cannot give the column first) and should be separated by a comma, as shown below:","metadata":{"id":"2NYv4_YSKO4f"}},{"cell_type":"code","source":"fifth_id = df.loc[4,\"id\"]\nprint(fifth_id)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81},"id":"iSrJcPshKg1N","outputId":"ec9e7a73-ce95-4187-f812-1266f6ab22a7"},"execution_count":16,"outputs":[{"name":"stdout","output_type":"stream","text":"10582676\n"}]},{"cell_type":"markdown","source":"Finally, the same syntax can be used to select multiple rows **and** columns at the same time; you simply need to provide a list or range of rows, followed by a list of column names, separated by a comma:","metadata":{}},{"cell_type":"code","source":"multi_row_and_col = df.loc[0:3,[\"id\", \"creation_time\"]]\ndisplay(multi_row_and_col)","metadata":{},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>creation_time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>24752851</td>\n      <td>2014-07-15 07:58:49</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>39926330</td>\n      <td>2016-10-07 21:55:43</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>26762140</td>\n      <td>2014-11-05 16:26:24</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>29889423</td>\n      <td>2015-04-27 07:14:00</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"         id        creation_time\n0  24752851  2014-07-15 07:58:49\n1  39926330  2016-10-07 21:55:43\n2  26762140  2014-11-05 16:26:24\n3  29889423  2015-04-27 07:14:00"},"metadata":{}}]},{"cell_type":"markdown","source":"## Exercise 1\n\nWhat is the tag of the 50th question in the data set?","metadata":{"id":"CqhRO0k3BvM2"}},{"cell_type":"code","source":"print(df.loc[49,\"tag\"])","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81},"id":"qAnp6yhjCL68","outputId":"f91db23c-48da-44c5-c084-00d64fb109f3"},"execution_count":18,"outputs":[{"name":"stdout","output_type":"stream","text":".htaccess\n"}]},{"cell_type":"markdown","source":"## 6. Summarizing column data\nPreviously, we sanity checked our data by checking its structure (data types and number of rows and columns). A second common step in preliminary exploration of the data is to examine individual columns (with the help of indexing, as covered previously). pandas can help with this process by offering several methods to *summarize* the data in a column.","metadata":{}},{"cell_type":"markdown","source":"### Summarizing numerical data\nSome columns contain numerical data. For example, years in the \"year\" column are represented as numbers. For numerical data, pandas offers a number of methods that implement common mathematical summary functions, such as finding the min and the max:","metadata":{}},{"cell_type":"code","source":"print(df[\"year\"].min()) # what's the earliest year in the data?\nprint(df[\"year\"].max()) # what's the latest year in the data?","metadata":{},"execution_count":38,"outputs":[{"name":"stdout","output_type":"stream","text":"2008\n2020\n"}]},{"cell_type":"markdown","source":"### Summarizing categorical data\nMathematical operations like min and max are useful for quickly summarizing numerical data. But not all columns are numerical. For example, the \"tag\" column contains text. But notice that the text in the \"text\" column is not just any arbitrary text! Rather, it seems to take on specific, repeated values, like \".htaccess\" or \"zxing\". Therefore, \"tag\" is what data scientists refer to as a *categorical variable*: one that can only take on values from a fixed, finite set of possibilities. To summarize categorical data, researchers generally want to know what is the set of values the variable can take? In pandas, this can be done using the `unique` method:","metadata":{}},{"cell_type":"code","source":"print(df[\"tag\"].unique())","metadata":{},"execution_count":23,"outputs":[{"name":"stdout","output_type":"stream","text":"['.htaccess' '.htpasswd' '.net' ... 'zurb-foundation' 'zurb-foundation-6'\n 'zxing']\n"}]},{"cell_type":"markdown","source":"## Exercise 2\n\n- Which years and months of questions are in the data?\n","metadata":{"id":"jRw50ceO_b_k"}},{"cell_type":"code","source":"print(df['year'].unique())\nprint(df['month'].unique())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IsL_ZU9BAiSY","outputId":"d68ca76a-96ab-4923-99be-bc769795b941"},"execution_count":24,"outputs":[{"name":"stdout","output_type":"stream","text":"[2014 2016 2015 2012 2017 2010 2019 2013 2018 2020 2011 2008 2009]\n[ 7 10 11  4  5  3 12  2  6  1  8  9]\n"}]},{"cell_type":"markdown","source":"## 7. Querying using conditions based on column values\n\nNow we can start looking at more advanced data analysis. Previously, we selected subsets of rows by giving the `loc` indexer a list or range of row indices. Most often, however, we don't know ahead of time which rows we want to analyze; instead, we more often want to select rows based on some conditions. For example, we might want all questions that took place in a specific year, or all questions that have a specific tag. To achieve this, the `loc` indexer can actually be used with conditions, not just with indices! \n\nThis is best explained through examples. Let's start with the first example question, looking for all questions that took place in a specific year, let's say 2008. The syntax for the comparison would be `df['year'] == 2008`, and we can pass this comparison directly to `loc`, as follows:","metadata":{"id":"vIQI7zsSC2FF"}},{"cell_type":"code","source":"year2008 = df.loc[df['year'] == 2008]\ndisplay(year2008)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"IHWjCNENBH43","outputId":"0d767471-f6e7-4604-bd4c-dad13ad54516"},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>creation_time</th>\n      <th>tag</th>\n      <th>year</th>\n      <th>month</th>\n      <th>accepted_id</th>\n      <th>owner_user_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>92</th>\n      <td>126868</td>\n      <td>2008-09-24 12:35:20</td>\n      <td>.net</td>\n      <td>2008</td>\n      <td>9</td>\n      <td>126910.0</td>\n      <td>369.0</td>\n    </tr>\n    <tr>\n      <th>110</th>\n      <td>301290</td>\n      <td>2008-11-19 08:15:01</td>\n      <td>.net</td>\n      <td>2008</td>\n      <td>11</td>\n      <td>301333.0</td>\n      <td>35012.0</td>\n    </tr>\n    <tr>\n      <th>186</th>\n      <td>36014</td>\n      <td>2008-08-30 14:57:13</td>\n      <td>.net</td>\n      <td>2008</td>\n      <td>8</td>\n      <td>41208.0</td>\n      <td>3347.0</td>\n    </tr>\n    <tr>\n      <th>227</th>\n      <td>191812</td>\n      <td>2008-10-10 15:16:30</td>\n      <td>.net</td>\n      <td>2008</td>\n      <td>10</td>\n      <td>193144.0</td>\n      <td>11356.0</td>\n    </tr>\n    <tr>\n      <th>249</th>\n      <td>50046</td>\n      <td>2008-09-08 16:05:17</td>\n      <td>.net</td>\n      <td>2008</td>\n      <td>9</td>\n      <td>50057.0</td>\n      <td>770.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>58981</th>\n      <td>235700</td>\n      <td>2008-10-25 00:44:49</td>\n      <td>winforms</td>\n      <td>2008</td>\n      <td>10</td>\n      <td>235721.0</td>\n      <td>22437.0</td>\n    </tr>\n    <tr>\n      <th>59011</th>\n      <td>18292</td>\n      <td>2008-08-20 16:13:39</td>\n      <td>winscp</td>\n      <td>2008</td>\n      <td>8</td>\n      <td>18370.0</td>\n      <td>1946.0</td>\n    </tr>\n    <tr>\n      <th>59040</th>\n      <td>160216</td>\n      <td>2008-10-01 23:25:43</td>\n      <td>wolfram-mathematica</td>\n      <td>2008</td>\n      <td>10</td>\n      <td>4700704.0</td>\n      <td>4234.0</td>\n    </tr>\n    <tr>\n      <th>59291</th>\n      <td>377470</td>\n      <td>2008-12-18 10:28:17</td>\n      <td>workflow</td>\n      <td>2008</td>\n      <td>12</td>\n      <td>NaN</td>\n      <td>47349.0</td>\n    </tr>\n    <tr>\n      <th>60316</th>\n      <td>317828</td>\n      <td>2008-11-25 15:59:31</td>\n      <td>xslt</td>\n      <td>2008</td>\n      <td>11</td>\n      <td>317839.0</td>\n      <td>5653.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>141 rows × 7 columns</p>\n</div>","text/plain":"           id        creation_time                  tag  year  month  \\\n92     126868  2008-09-24 12:35:20                 .net  2008      9   \n110    301290  2008-11-19 08:15:01                 .net  2008     11   \n186     36014  2008-08-30 14:57:13                 .net  2008      8   \n227    191812  2008-10-10 15:16:30                 .net  2008     10   \n249     50046  2008-09-08 16:05:17                 .net  2008      9   \n...       ...                  ...                  ...   ...    ...   \n58981  235700  2008-10-25 00:44:49             winforms  2008     10   \n59011   18292  2008-08-20 16:13:39               winscp  2008      8   \n59040  160216  2008-10-01 23:25:43  wolfram-mathematica  2008     10   \n59291  377470  2008-12-18 10:28:17             workflow  2008     12   \n60316  317828  2008-11-25 15:59:31                 xslt  2008     11   \n\n       accepted_id  owner_user_id  \n92        126910.0          369.0  \n110       301333.0        35012.0  \n186        41208.0         3347.0  \n227       193144.0        11356.0  \n249        50057.0          770.0  \n...            ...            ...  \n58981     235721.0        22437.0  \n59011      18370.0         1946.0  \n59040    4700704.0         4234.0  \n59291          NaN        47349.0  \n60316     317839.0         5653.0  \n\n[141 rows x 7 columns]"},"metadata":{}}]},{"cell_type":"markdown","source":"Notice that this syntax is similar to the filtering syntax used by numpy (as seen in the previous Python workshop) and by R. Alternatively, if you are familiar with SQL, you can think of the above code as being equivalent to the SQL query `select * from df where year=2008`.\n\nLike in numpy, you can include multiple comparisons in a single query, combining them using operators `&` for \"and\" and `|` for \"or\". So, for example, if we wanted only questions from August 2008:","metadata":{}},{"cell_type":"code","source":"august2008 = df.loc[(df['year']==2008)&(df['month']==8)]\ndisplay(august2008)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"B4xk3-AJDAPA","outputId":"c1994fa7-33c0-4c39-b1d4-b52ee8a2f112"},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>creation_time</th>\n      <th>tag</th>\n      <th>year</th>\n      <th>month</th>\n      <th>accepted_id</th>\n      <th>owner_user_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>186</th>\n      <td>36014</td>\n      <td>2008-08-30 14:57:13</td>\n      <td>.net</td>\n      <td>2008</td>\n      <td>8</td>\n      <td>41208.0</td>\n      <td>3347.0</td>\n    </tr>\n    <tr>\n      <th>385</th>\n      <td>8142</td>\n      <td>2008-08-11 19:50:31</td>\n      <td>.net</td>\n      <td>2008</td>\n      <td>8</td>\n      <td>8152.0</td>\n      <td>380.0</td>\n    </tr>\n    <tr>\n      <th>5888</th>\n      <td>6719</td>\n      <td>2008-08-09 09:51:37</td>\n      <td>asp.net-2.0</td>\n      <td>2008</td>\n      <td>8</td>\n      <td>6720.0</td>\n      <td>133.0</td>\n    </tr>\n    <tr>\n      <th>19152</th>\n      <td>16233</td>\n      <td>2008-08-19 14:13:18</td>\n      <td>fonts</td>\n      <td>2008</td>\n      <td>8</td>\n      <td>16255.0</td>\n      <td>1000.0</td>\n    </tr>\n    <tr>\n      <th>30592</th>\n      <td>26620</td>\n      <td>2008-08-25 18:45:27</td>\n      <td>jquery</td>\n      <td>2008</td>\n      <td>8</td>\n      <td>26681.0</td>\n      <td>2138.0</td>\n    </tr>\n    <tr>\n      <th>33164</th>\n      <td>1005</td>\n      <td>2008-08-04 03:34:02</td>\n      <td>linux</td>\n      <td>2008</td>\n      <td>8</td>\n      <td>37042.0</td>\n      <td>85.0</td>\n    </tr>\n    <tr>\n      <th>33260</th>\n      <td>263</td>\n      <td>2008-08-01 23:27:24</td>\n      <td>linux</td>\n      <td>2008</td>\n      <td>8</td>\n      <td>607.0</td>\n      <td>61.0</td>\n    </tr>\n    <tr>\n      <th>35715</th>\n      <td>20389</td>\n      <td>2008-08-21 16:12:45</td>\n      <td>moss</td>\n      <td>2008</td>\n      <td>8</td>\n      <td>33052.0</td>\n      <td>1453.0</td>\n    </tr>\n    <tr>\n      <th>36128</th>\n      <td>2056</td>\n      <td>2008-08-05 10:06:33</td>\n      <td>mvp</td>\n      <td>2008</td>\n      <td>8</td>\n      <td>101561.0</td>\n      <td>358.0</td>\n    </tr>\n    <tr>\n      <th>45334</th>\n      <td>4565</td>\n      <td>2008-08-07 10:20:16</td>\n      <td>qos</td>\n      <td>2008</td>\n      <td>8</td>\n      <td>NaN</td>\n      <td>486.0</td>\n    </tr>\n    <tr>\n      <th>47327</th>\n      <td>32941</td>\n      <td>2008-08-28 18:03:36</td>\n      <td>reporting-services</td>\n      <td>2008</td>\n      <td>8</td>\n      <td>344461.0</td>\n      <td>2184.0</td>\n    </tr>\n    <tr>\n      <th>53315</th>\n      <td>16142</td>\n      <td>2008-08-19 13:22:03</td>\n      <td>svn</td>\n      <td>2008</td>\n      <td>8</td>\n      <td>16163.0</td>\n      <td>914.0</td>\n    </tr>\n    <tr>\n      <th>59011</th>\n      <td>18292</td>\n      <td>2008-08-20 16:13:39</td>\n      <td>winscp</td>\n      <td>2008</td>\n      <td>8</td>\n      <td>18370.0</td>\n      <td>1946.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"          id        creation_time                 tag  year  month  \\\n186    36014  2008-08-30 14:57:13                .net  2008      8   \n385     8142  2008-08-11 19:50:31                .net  2008      8   \n5888    6719  2008-08-09 09:51:37         asp.net-2.0  2008      8   \n19152  16233  2008-08-19 14:13:18               fonts  2008      8   \n30592  26620  2008-08-25 18:45:27              jquery  2008      8   \n33164   1005  2008-08-04 03:34:02               linux  2008      8   \n33260    263  2008-08-01 23:27:24               linux  2008      8   \n35715  20389  2008-08-21 16:12:45                moss  2008      8   \n36128   2056  2008-08-05 10:06:33                 mvp  2008      8   \n45334   4565  2008-08-07 10:20:16                 qos  2008      8   \n47327  32941  2008-08-28 18:03:36  reporting-services  2008      8   \n53315  16142  2008-08-19 13:22:03                 svn  2008      8   \n59011  18292  2008-08-20 16:13:39              winscp  2008      8   \n\n       accepted_id  owner_user_id  \n186        41208.0         3347.0  \n385         8152.0          380.0  \n5888        6720.0          133.0  \n19152      16255.0         1000.0  \n30592      26681.0         2138.0  \n33164      37042.0           85.0  \n33260        607.0           61.0  \n35715      33052.0         1453.0  \n36128     101561.0          358.0  \n45334          NaN          486.0  \n47327     344461.0         2184.0  \n53315      16163.0          914.0  \n59011      18370.0         1946.0  "},"metadata":{}}]},{"cell_type":"markdown","source":"Notice that when we run a query, the matching rows get returned in a new DataFrame, which is a subset of the original, containing only rows that matched the query. We can confirm that the result is still a DataFrame by checking the type of the variable:","metadata":{}},{"cell_type":"code","source":"type(august2008)","metadata":{},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"pandas.core.frame.DataFrame"},"metadata":{}}]},{"cell_type":"markdown","source":"This means that we can run all of the previously described DataFrame methods on the query results! For example, this can be useful for finding out how many rows matched the query:","metadata":{}},{"cell_type":"code","source":"print(len(august2008.index)) # we can use the same code we saw earlier for counting rows, because august2008 is still a DataFrame","metadata":{},"execution_count":28,"outputs":[{"name":"stdout","output_type":"stream","text":"13\n"}]},{"cell_type":"markdown","source":"## Exercise 3\n\nHow many questions in 2013 had the tag \"python\"?","metadata":{}},{"cell_type":"code","source":"python2013 = df.loc[(df['year']==2013)&(df['tag']=='python')]\nprint(len(python2013.index))","metadata":{},"execution_count":29,"outputs":[{"name":"stdout","output_type":"stream","text":"89\n"}]},{"cell_type":"markdown","source":"## 8. Modifying and adding rows\nQueries aren't just useful for accessing data; we may sometimes also want to *modify* data that matches a query! One common reason to do this is for data cleaning. For example, some questions in our data set have the tag \".net\" and others have the tag \"net\", but both of these actually refer to the same thing (the Microsoft dotNET framework). So when we clean the data, we might want to standardize all instances of \".net\" to \"net\". This can be done with the following steps:\n\n1. Write a query to find all instances of the thing you want to clean (in this case, questions with the tag \".net\") (See Section 7)\n2. Use row-and-column indexing to combine this query (which selects certain rows) with column selection (See Section 5)\n3. Use Python assigment syntax (the \"=\" operator) to set the new value\n\nAgain, this is best understood through an example. The following code implements all the steps to standardize \".net\" to \"net\":","metadata":{"id":"yZ-cXW6P6Opk"}},{"cell_type":"code","source":"# Convert .net to net\ndf.loc[(df['tag'] == '.net'), 'tag'] = 'net'\n# to confirm that it worked, count how many questions are now tagged \"net\" and how many are tagged \".net\" (the latter should be zero)\nprint(len(df.loc[df['tag'] == 'net'].index))\nprint(len(df.loc[df['tag'] == '.net'].index))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":511},"id":"EVP5mQ1R6Tl6","outputId":"c3a2e13f-6744-4c6b-8ae4-f251dcc7db4e"},"execution_count":31,"outputs":[{"name":"stdout","output_type":"stream","text":"306\n0\n"}]},{"cell_type":"markdown","source":"In this example, we *replaced* values in an existing column. But there are other cases where instead, you might want to preserve the original data, and instead put the cleaned data in a new column. pandas lets us do this too! As an example, let's recall the missing values in the \"accepted_id\" column that we told you about earlier. Remember that the interpretation of these missing values is that the question did not receive an accepted answer. For some research questions, we might simply want to know *whether* a question received an accepted answer or not, without caring about the ID of the answer. But this does not mean we want to completely replace the \"accepted_id\" column, because for *other* research questions we might still care about the answer IDs! So instead, we might want to create a *new* column that contains the simplified information of whether a question received an accepted answer.\n\nWe will start by creating a new column, let's call it \"has_answer\". Initially, we'll create the column as a copy of \"accepted_id\"; we can then modify it (using code similar to what we did previously for \"tag\") without affecting the original column. To create a new column, we can simply use Python assigment syntax:","metadata":{}},{"cell_type":"code","source":"df[\"has_answer\"] = df[\"accepted_id\"]\ndisplay(df)","metadata":{},"execution_count":35,"outputs":[{"output_type":"display_data","data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>creation_time</th>\n      <th>tag</th>\n      <th>year</th>\n      <th>month</th>\n      <th>accepted_id</th>\n      <th>owner_user_id</th>\n      <th>has_answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>24752851</td>\n      <td>2014-07-15 07:58:49</td>\n      <td>.htaccess</td>\n      <td>2014</td>\n      <td>7</td>\n      <td>24768174.0</td>\n      <td>509916.0</td>\n      <td>24768174.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>39926330</td>\n      <td>2016-10-07 21:55:43</td>\n      <td>.htaccess</td>\n      <td>2016</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>5281549.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>26762140</td>\n      <td>2014-11-05 16:26:24</td>\n      <td>.htaccess</td>\n      <td>2014</td>\n      <td>11</td>\n      <td>26762882.0</td>\n      <td>2091861.0</td>\n      <td>26762882.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>29889423</td>\n      <td>2015-04-27 07:14:00</td>\n      <td>.htaccess</td>\n      <td>2015</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>589973.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10582676</td>\n      <td>2012-05-14 11:43:27</td>\n      <td>.htaccess</td>\n      <td>2012</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>1326187.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>60565</th>\n      <td>39093361</td>\n      <td>2016-08-23 05:55:50</td>\n      <td>zurb-foundation</td>\n      <td>2016</td>\n      <td>8</td>\n      <td>NaN</td>\n      <td>2729887.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>60566</th>\n      <td>37248756</td>\n      <td>2016-05-16 07:25:11</td>\n      <td>zurb-foundation-6</td>\n      <td>2016</td>\n      <td>5</td>\n      <td>37251132.0</td>\n      <td>1148107.0</td>\n      <td>37251132.0</td>\n    </tr>\n    <tr>\n      <th>60567</th>\n      <td>16264645</td>\n      <td>2013-04-28 15:50:01</td>\n      <td>zxing</td>\n      <td>2013</td>\n      <td>4</td>\n      <td>16264687.0</td>\n      <td>2102871.0</td>\n      <td>16264687.0</td>\n    </tr>\n    <tr>\n      <th>60568</th>\n      <td>30389191</td>\n      <td>2015-05-22 05:46:07</td>\n      <td>zxing</td>\n      <td>2015</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>571433.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>60569</th>\n      <td>26181174</td>\n      <td>2014-10-03 14:44:59</td>\n      <td>zxing</td>\n      <td>2014</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>1491212.0</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>60570 rows × 8 columns</p>\n</div>","text/plain":"             id        creation_time                tag  year  month  \\\n0      24752851  2014-07-15 07:58:49          .htaccess  2014      7   \n1      39926330  2016-10-07 21:55:43          .htaccess  2016     10   \n2      26762140  2014-11-05 16:26:24          .htaccess  2014     11   \n3      29889423  2015-04-27 07:14:00          .htaccess  2015      4   \n4      10582676  2012-05-14 11:43:27          .htaccess  2012      5   \n...         ...                  ...                ...   ...    ...   \n60565  39093361  2016-08-23 05:55:50    zurb-foundation  2016      8   \n60566  37248756  2016-05-16 07:25:11  zurb-foundation-6  2016      5   \n60567  16264645  2013-04-28 15:50:01              zxing  2013      4   \n60568  30389191  2015-05-22 05:46:07              zxing  2015      5   \n60569  26181174  2014-10-03 14:44:59              zxing  2014     10   \n\n       accepted_id  owner_user_id  has_answer  \n0       24768174.0       509916.0  24768174.0  \n1              NaN      5281549.0         NaN  \n2       26762882.0      2091861.0  26762882.0  \n3              NaN       589973.0         NaN  \n4              NaN      1326187.0         NaN  \n...            ...            ...         ...  \n60565          NaN      2729887.0         NaN  \n60566   37251132.0      1148107.0  37251132.0  \n60567   16264687.0      2102871.0  16264687.0  \n60568          NaN       571433.0         NaN  \n60569          NaN      1491212.0         NaN  \n\n[60570 rows x 8 columns]"},"metadata":{}}]},{"cell_type":"markdown","source":"We see that there is now a new column \"has_answer\" that is an exact copy of \"accepted_id\". Next we will modify it as follows: rows containing missing values will be set to 0 (representing \"no answer\") and the rest will be set to \"1\" (representing \"has answer\").\n\nSyntax notes: to check if a value is missing, pandas provides the special comparison function `isna`. To negate a comparison (so we can say when a row does **not** contain a missing value), we'll use the the `~` operator.","metadata":{}},{"cell_type":"code","source":"df.loc[~(pd.isna(df['has_answer'])), 'has_answer'] = 1\ndf.loc[(pd.isna(df['has_answer'])), 'has_answer'] = 0\ndisplay(df)","metadata":{},"execution_count":37,"outputs":[{"output_type":"display_data","data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>creation_time</th>\n      <th>tag</th>\n      <th>year</th>\n      <th>month</th>\n      <th>accepted_id</th>\n      <th>owner_user_id</th>\n      <th>has_answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>24752851</td>\n      <td>2014-07-15 07:58:49</td>\n      <td>.htaccess</td>\n      <td>2014</td>\n      <td>7</td>\n      <td>24768174.0</td>\n      <td>509916.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>39926330</td>\n      <td>2016-10-07 21:55:43</td>\n      <td>.htaccess</td>\n      <td>2016</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>5281549.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>26762140</td>\n      <td>2014-11-05 16:26:24</td>\n      <td>.htaccess</td>\n      <td>2014</td>\n      <td>11</td>\n      <td>26762882.0</td>\n      <td>2091861.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>29889423</td>\n      <td>2015-04-27 07:14:00</td>\n      <td>.htaccess</td>\n      <td>2015</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>589973.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10582676</td>\n      <td>2012-05-14 11:43:27</td>\n      <td>.htaccess</td>\n      <td>2012</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>1326187.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>60565</th>\n      <td>39093361</td>\n      <td>2016-08-23 05:55:50</td>\n      <td>zurb-foundation</td>\n      <td>2016</td>\n      <td>8</td>\n      <td>NaN</td>\n      <td>2729887.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>60566</th>\n      <td>37248756</td>\n      <td>2016-05-16 07:25:11</td>\n      <td>zurb-foundation-6</td>\n      <td>2016</td>\n      <td>5</td>\n      <td>37251132.0</td>\n      <td>1148107.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>60567</th>\n      <td>16264645</td>\n      <td>2013-04-28 15:50:01</td>\n      <td>zxing</td>\n      <td>2013</td>\n      <td>4</td>\n      <td>16264687.0</td>\n      <td>2102871.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>60568</th>\n      <td>30389191</td>\n      <td>2015-05-22 05:46:07</td>\n      <td>zxing</td>\n      <td>2015</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>571433.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>60569</th>\n      <td>26181174</td>\n      <td>2014-10-03 14:44:59</td>\n      <td>zxing</td>\n      <td>2014</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>1491212.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>60570 rows × 8 columns</p>\n</div>","text/plain":"             id        creation_time                tag  year  month  \\\n0      24752851  2014-07-15 07:58:49          .htaccess  2014      7   \n1      39926330  2016-10-07 21:55:43          .htaccess  2016     10   \n2      26762140  2014-11-05 16:26:24          .htaccess  2014     11   \n3      29889423  2015-04-27 07:14:00          .htaccess  2015      4   \n4      10582676  2012-05-14 11:43:27          .htaccess  2012      5   \n...         ...                  ...                ...   ...    ...   \n60565  39093361  2016-08-23 05:55:50    zurb-foundation  2016      8   \n60566  37248756  2016-05-16 07:25:11  zurb-foundation-6  2016      5   \n60567  16264645  2013-04-28 15:50:01              zxing  2013      4   \n60568  30389191  2015-05-22 05:46:07              zxing  2015      5   \n60569  26181174  2014-10-03 14:44:59              zxing  2014     10   \n\n       accepted_id  owner_user_id  has_answer  \n0       24768174.0       509916.0         1.0  \n1              NaN      5281549.0         0.0  \n2       26762882.0      2091861.0         1.0  \n3              NaN       589973.0         0.0  \n4              NaN      1326187.0         0.0  \n...            ...            ...         ...  \n60565          NaN      2729887.0         0.0  \n60566   37251132.0      1148107.0         1.0  \n60567   16264687.0      2102871.0         1.0  \n60568          NaN       571433.0         0.0  \n60569          NaN      1491212.0         0.0  \n\n[60570 rows x 8 columns]"},"metadata":{}}]},{"cell_type":"markdown","source":"## 9. Fine-grained analysis using groupby\nIn section 6, we showed how to summarize data from an *entire* column, such as taking the min and max of numerical data. This is useful for initial exploration, but when tackling actual research questions, we often want to do more complicated operations involving interactions between multiple variables. For example, instead of just finding the earliest year in the data, we might be interested in finding the earliest year each tag first appeared. Using what we have learned so far, one way to do this would be to use queries: for each tag, you could write a query to select only the rows containing that tag, and call the `min` method each time. But this is extremely tedious, given that there are so many tags in the data! Thankfully, pandas offers a faster alternative: `groupby`.\n\nThe `groupby` method can be thought of a splitting a DataFrame based on some categorical variable. For example, if we use `groupby` on the \"tag\" column, we'll have one group containing rows tagged \".htaccess\", another containing rows tagged \"python\", and so on, for every unique tag. Now, if we just run `groupby` on its own, we won't immediately see anything useful:","metadata":{}},{"cell_type":"code","source":"df_grouped = df.groupby(\"tag\")\ndisplay(df_grouped) # prints out some strange code that isn't super useful...","metadata":{},"execution_count":41,"outputs":[{"output_type":"display_data","data":{"text/plain":"<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f428558f070>"},"metadata":{}}]},{"cell_type":"markdown","source":"But the key difference happens when we call summarization methods on the grouped variable. Instead of running the summarization on the entire data, as happens normally, the summarization will be run separately on each group! Let's try using the `min` summarization method on \"year\" in the grouped data:","metadata":{}},{"cell_type":"code","source":"display(df_grouped['year'].min()) # we can use the same indexing and summarization syntax as we did for regular DataFrames, but the operation now happens separately for each group!","metadata":{},"execution_count":44,"outputs":[{"output_type":"display_data","data":{"text/plain":"tag\n.htaccess            2010\n.htpasswd            2019\n.net-2.0             2011\n.net-3.5             2009\n.net-4.0             2009\n                     ... \nzsh                  2013\nzshrc                2014\nzurb-foundation      2013\nzurb-foundation-6    2016\nzxing                2013\nName: year, Length: 9033, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"As we can see, instead of a single minimum, we get multiple: one for each group! Specifically, we are seeing the earliest year each tag first appeared. We can immediately see how this might be useful for tracking trends; for example, questions tagged \".htaccess\" started in 2010, while questions tagged \".htpasswd\" didn't start until 2019. This might suggest, for example, that .htpasswd is a newer tag.","metadata":{}},{"cell_type":"markdown","source":"There are other kinds of summarization methods that can be useful when combined with `groupby`. Here are just a few examples:","metadata":{}},{"cell_type":"code","source":"# The count() method counts the total number of unique items in a column.\n# When combined with groupby, the counting will happen separately per group.\n# Let's try using it to find out how many questions happened per year (by counting the number of unique question IDs per year group)\ndf_grouped = df.groupby(\"year\")\ndisplay(df_grouped[\"id\"].count())","metadata":{},"execution_count":48,"outputs":[{"output_type":"display_data","data":{"text/plain":"year\n2008     141\n2009     957\n2010    1996\n2011    3408\n2012    4781\n2013    5994\n2014    6428\n2015    6583\n2016    6637\n2017    6216\n2018    5624\n2019    5330\n2020    6475\nName: id, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# The sum() method computes the sum of all items in a column\n# When combined with groupby, sums are computed separately per group\n# Let's try using it to find out how many questions were answered per year.\ndf_grouped = df.groupby(\"year\")\ndisplay(df_grouped[\"has_answer\"].sum())","metadata":{},"execution_count":49,"outputs":[{"output_type":"display_data","data":{"text/plain":"year\n2008     113.0\n2009     698.0\n2010    1462.0\n2011    2358.0\n2012    3143.0\n2013    3372.0\n2014    3436.0\n2015    3339.0\n2016    3189.0\n2017    2874.0\n2018    2494.0\n2019    2352.0\n2020    2314.0\nName: has_answer, dtype: float64"},"metadata":{}}]},{"cell_type":"markdown","source":"Finally, note that it is possible to group on multiple columns at once! This will create a group for each unique pair of possible values in the two columns. For example, we might be interested not just in years, but in specific year-month combinations:","metadata":{}},{"cell_type":"code","source":"df_grouped = df.groupby([\"year\", \"month\"])\ndisplay(df_grouped[\"id\"].count())","metadata":{},"execution_count":50,"outputs":[{"output_type":"display_data","data":{"text/plain":"year  month\n2008  8         13\n      9         37\n      10        34\n      11        26\n      12        31\n              ... \n2020  8        529\n      9        555\n      10       540\n      11       510\n      12       106\nName: id, Length: 149, dtype: int64"},"metadata":{}}]}]}